{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4stAYxWm8w-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e51e381-2438-4d32-bc20-ece43a1022da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###!@0 START INIT ENVIRONMENT\n",
        "!ls /content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf /content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.2-bin-hadoop3\"\n",
        "###!@0 END INIT ENVIRONMENT"
      ],
      "metadata": {
        "id": "-Agf_0iH8zum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7427b7f-33b8-4867-c203-ba2bac0a4b01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/spark-3.5.2-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "!rm -rf /content/data/*.csv\n",
        "!ln -s /content/drive/MyDrive/DES_Project/DataSet/weather_data/*.csv /content/data/"
      ],
      "metadata": {
        "id": "rJI28leQ9PwO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Weather and consumption Data\n",
        "\n",
        "csv_path1 = '/content/drive/MyDrive/DES_Project/DataSet/weather_data.csv'\n",
        "csv_path2 = '/content/drive/MyDrive/DES_Project/DataSet/electricity_consumption_delhi.csv'"
      ],
      "metadata": {
        "id": "JpYLNkidfXci"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "input_type = 'sample'\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"Colab\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "# Spark is ready to go within Colab!\n",
        "###!@1 END OF PYSPARK INIT"
      ],
      "metadata": {
        "id": "RgVoPUE0_Wpm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data from csv files\n",
        "\n",
        "weather_data = spark.read.csv(csv_path1, header=True, inferSchema=True)\n",
        "electricity_data = spark.read.csv(csv_path2, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "AJpBJ49vw2wV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "input_type = 'sample'\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"Colab\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "# Spark is ready to go within Colab!\n",
        "###!@1 END OF PYSPARK INIT"
      ],
      "metadata": {
        "id": "EgkS57_79Rii"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required Libraries\n",
        "\n",
        "from pyspark.sql.functions import col, count\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import avg, date_trunc"
      ],
      "metadata": {
        "id": "BWuojU6D9bVw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows\n",
        "\n",
        "electricity_data.show(5)\n",
        "weather_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-GbRuA4hy-b",
        "outputId": "adcf3ff6-26a7-4e12-878b-ddacf2d19a73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+----------------+\n",
            "| City|               Date|Consumption (MW)|\n",
            "+-----+-------------------+----------------+\n",
            "|Delhi|1979-01-01 00:00:00|              50|\n",
            "|Delhi|1979-01-01 00:01:00|              61|\n",
            "|Delhi|1979-01-01 00:02:00|              50|\n",
            "|Delhi|1979-01-01 00:03:00|              50|\n",
            "|Delhi|1979-01-01 00:04:00|              50|\n",
            "+-----+-------------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "| City|               Date|Temperature (C)|Feels Like (C)|Humidity (%)|Pressure (hPa)|Weather Description|Wind Speed (m/s)|Cloudiness (%)|Rain (1h mm)|            Sunrise|             Sunset|\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "|Delhi|1979-01-01 00:00:00|           9.26|          7.33|          28|          1086|              clear|            2.42|             0|        3.91|1979-01-01 06:29:00|1979-01-01 18:31:00|\n",
            "|Delhi|1979-01-01 00:01:00|          14.96|         15.25|          68|           930|               cold|            1.23|            49|        5.16|1979-01-01 06:30:00|1979-01-01 18:31:00|\n",
            "|Delhi|1979-01-01 00:02:00|           3.93|           2.0|          33|           990|              windy|            0.57|            33|        5.16|1979-01-01 06:28:00|1979-01-01 18:28:00|\n",
            "|Delhi|1979-01-01 00:03:00|          11.46|         10.71|          35|          1034|              windy|            3.55|            48|         0.6|1979-01-01 06:32:00|1979-01-01 18:32:00|\n",
            "|Delhi|1979-01-01 00:04:00|           10.6|          9.25|          56|           965|              sunny|            4.06|            70|        4.89|1979-01-01 06:28:00|1979-01-01 18:29:00|\n",
            "+-----+-------------------+---------------+--------------+------------+--------------+-------------------+----------------+--------------+------------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema\n",
        "\n",
        "electricity_data.printSchema()\n",
        "weather_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGeaw2SRh1At",
        "outputId": "3fab2f1a-5b71-4185-d11c-59e4ebaf37df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Consumption (MW): integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Temperature (C): double (nullable = true)\n",
            " |-- Feels Like (C): double (nullable = true)\n",
            " |-- Humidity (%): integer (nullable = true)\n",
            " |-- Pressure (hPa): integer (nullable = true)\n",
            " |-- Weather Description: string (nullable = true)\n",
            " |-- Wind Speed (m/s): double (nullable = true)\n",
            " |-- Cloudiness (%): integer (nullable = true)\n",
            " |-- Rain (1h mm): double (nullable = true)\n",
            " |-- Sunrise: timestamp (nullable = true)\n",
            " |-- Sunset: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check data statistics\n",
        "\n",
        "electricity_data.describe().show()\n",
        "weather_data.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2sWdxXph3LH",
        "outputId": "16204474-7597-4f40-e044-8dd73deeac5f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------------------+\n",
            "|summary|    City|  Consumption (MW)|\n",
            "+-------+--------+------------------+\n",
            "|  count|24144480|          24144480|\n",
            "|   mean|    NULL|3242.0363132277025|\n",
            "| stddev|    NULL|2824.2361845591695|\n",
            "|    min|   Delhi|                50|\n",
            "|    max|   Delhi|              8600|\n",
            "+-------+--------+------------------+\n",
            "\n",
            "+-------+--------+------------------+------------------+------------------+----------------+-------------------+------------------+------------------+------------------+\n",
            "|summary|    City|   Temperature (C)|    Feels Like (C)|      Humidity (%)|  Pressure (hPa)|Weather Description|  Wind Speed (m/s)|    Cloudiness (%)|      Rain (1h mm)|\n",
            "+-------+--------+------------------+------------------+------------------+----------------+-------------------+------------------+------------------+------------------+\n",
            "|  count|24111360|          24111360|          24111360|          24111360|        24111360|           24111360|          24111360|          24111360|          24111360|\n",
            "|   mean|    NULL|27.118999642077316|27.118884437045253|50.001656107328664|999.997929938419|               NULL|2.5496616246451653|40.003838522588524| 2.516733689016232|\n",
            "| stddev|    NULL|11.092463638740588|11.152331737035675|13.290922530379422| 58.009891692574|               NULL| 1.414364506238141|   32.863833642307|1.5248897046719203|\n",
            "|    min|   Delhi|               2.0|               0.0|                20|             900|              clear|               0.1|                 0|               0.0|\n",
            "|    max|   Delhi|              45.0|              47.0|                80|            1100|              windy|               5.0|               100|               6.0|\n",
            "+-------+--------+------------------+------------------+------------------+----------------+-------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "\n",
        "def missing_values(df):\n",
        "    return df.select([(count(col(c)) / df.count()).alias(c + \"_missing\") for c in df.columns])\n",
        "\n",
        "missing_values(electricity_data).show()\n",
        "missing_values(weather_data).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM8BIN5ih5WJ",
        "outputId": "ea0c98ee-66c4-4578-9147-fdf8bc69c5e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+------------------------+\n",
            "|City_missing|Date_missing|Consumption (MW)_missing|\n",
            "+------------+------------+------------------------+\n",
            "|         1.0|         1.0|                     1.0|\n",
            "+------------+------------+------------------------+\n",
            "\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "|City_missing|Date_missing|Temperature (C)_missing|Feels Like (C)_missing|Humidity (%)_missing|Pressure (hPa)_missing|Weather Description_missing|Wind Speed (m/s)_missing|Cloudiness (%)_missing|Rain (1h mm)_missing|Sunrise_missing|Sunset_missing|\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "|         1.0|         1.0|                    1.0|                   1.0|                 1.0|                   1.0|                        1.0|                     1.0|                   1.0|                 1.0|            1.0|           1.0|\n",
            "+------------+------------+-----------------------+----------------------+--------------------+----------------------+---------------------------+------------------------+----------------------+--------------------+---------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, date_trunc\n",
        "\n",
        "# Truncate the timestamp to get the day\n",
        "electricity_data = electricity_data.withColumn(\"Date\", date_trunc(\"day\", col(\"Date\")))\n",
        "\n",
        "# Group by day and compute the daily average of Consumption (MW)\n",
        "daily_electricity_data = electricity_data.groupBy(\"Date\").agg(avg(\"Consumption (MW)\").alias(\"Daily_Consumption\"))\n",
        "\n",
        "daily_electricity_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHXulgMF7xCo",
        "outputId": "dd2a181a-8f07-49bc-bba9-9f9b4568f744"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+\n",
            "|               Date|Daily_Consumption|\n",
            "+-------------------+-----------------+\n",
            "|1979-10-25 00:00:00|74.61736111111111|\n",
            "|1980-07-22 00:00:00|         75.28125|\n",
            "|1981-07-21 00:00:00|74.30069444444445|\n",
            "|1982-02-16 00:00:00|76.12638888888888|\n",
            "|1982-05-19 00:00:00|75.54166666666667|\n",
            "+-------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate the timestamp to get the day\n",
        "weather_data = weather_data.withColumn(\"Date\", date_trunc(\"day\", col(\"Date\")))\n",
        "\n",
        "# Group by day and compute daily averages for relevant columns\n",
        "daily_weather_data = weather_data.groupBy(\"Date\").agg(\n",
        "    avg(\"Temperature (C)\").alias(\"Avg_Temperature\"),\n",
        "    avg(\"Humidity (%)\").alias(\"Avg_Humidity\"),\n",
        "    avg(\"Wind Speed (m/s)\").alias(\"Avg_Wind_Speed\"),\n",
        "    avg(\"Cloudiness (%)\").alias(\"Avg_Cloudiness\"),\n",
        "    avg(\"Rain (1h mm)\").alias(\"Avg_Rain\")\n",
        ")\n",
        "\n",
        "daily_weather_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Meg4F9M7zUq",
        "outputId": "80154798-bc2c-4fb6-9a20-a9d02ca7905e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|               Date|   Avg_Temperature|      Avg_Humidity|    Avg_Wind_Speed|    Avg_Cloudiness|          Avg_Rain|\n",
            "+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|1979-10-25 00:00:00|30.020722222222183| 50.16180555555555|2.5672430555555574| 39.44444444444444|2.5462638888888853|\n",
            "|1980-07-22 00:00:00| 30.01484027777782| 49.99861111111111|2.5423055555555516|40.108333333333334|2.4898888888888906|\n",
            "|1979-01-10 00:00:00| 13.71720833333335|50.421527777777776| 2.571326388888892| 38.93611111111111|2.4883402777777817|\n",
            "|1979-03-07 00:00:00| 37.47304166666668| 50.33611111111111|2.5971180555555575| 39.43402777777778| 2.461298611111108|\n",
            "|1979-04-03 00:00:00| 37.51625000000001| 49.94305555555555|2.5208263888888953|39.509027777777774|2.4597291666666665|\n",
            "+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join daily electricity data with daily weather data on Date\n",
        "daily_data = daily_electricity_data.join(daily_weather_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "daily_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiGuAKiT71Qq",
        "outputId": "0ae51964-5335-410e-a60e-ab07923dc6ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|               Date|Daily_Consumption|   Avg_Temperature|      Avg_Humidity|    Avg_Wind_Speed|    Avg_Cloudiness|          Avg_Rain|\n",
            "+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|1979-10-25 00:00:00|74.61736111111111|30.020722222222183| 50.16180555555555|2.5672430555555574| 39.44444444444444|2.5462638888888853|\n",
            "|1980-07-22 00:00:00|         75.28125| 30.01484027777782| 49.99861111111111|2.5423055555555516|40.108333333333334|2.4898888888888906|\n",
            "|1979-01-10 00:00:00|73.61805555555556| 13.71720833333335|50.421527777777776| 2.571326388888892| 38.93611111111111|2.4883402777777817|\n",
            "|1979-03-07 00:00:00|75.41388888888889| 37.47304166666668| 50.33611111111111|2.5971180555555575| 39.43402777777778| 2.461298611111108|\n",
            "|1979-04-03 00:00:00|74.28055555555555| 37.51625000000001| 49.94305555555555|2.5208263888888953|39.509027777777774|2.4597291666666665|\n",
            "+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features\n",
        "feature_columns = [\"Avg_Temperature\", \"Avg_Humidity\", \"Avg_Wind_Speed\", \"Avg_Cloudiness\", \"Avg_Rain\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Transform dataset\n",
        "final_daily_data = assembler.transform(daily_data).select(\"features\", \"Daily_Consumption\")"
      ],
      "metadata": {
        "id": "xET0OnOa74Xs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "train_data, test_data = final_daily_data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "pz0OuyHisifL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Daily_Consumption\", maxIter=50)\n",
        "model = gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "QOchMaH8sl7Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = RegressionEvaluator(labelCol=\"Daily_Consumption\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Qo0mu3sn4P",
        "outputId": "e17e82e1-b4de-4cb3-acf7-afe48ef181a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 2901.5879574995583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert date columns to proper format\n",
        "\n",
        "# weather_data = weather_data.withColumn(\"Date\", to_date(\"Date\"))\n",
        "# electricity_data = electricity_data.withColumn(\"Date\", to_date(\"Date\"))\n",
        "\n",
        "# # Join datasets on the date column\n",
        "# data = electricity_data.join(weather_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# # Drop one of the city columns\n",
        "# data = data.drop(weather_data[\"city\"])\n",
        "\n",
        "# data = data.withColumn(\"Date\", to_date(col(\"Date\")))\n",
        "\n",
        "# data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09OLT1syh69_",
        "outputId": "5c2b3e6d-9a07-4c9d-86e9-d644457a851f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Consumption (MW): integer (nullable = true)\n",
            " |-- Temperature (C): double (nullable = true)\n",
            " |-- Feels Like (C): double (nullable = true)\n",
            " |-- Humidity (%): integer (nullable = true)\n",
            " |-- Pressure (hPa): integer (nullable = true)\n",
            " |-- Weather Description: string (nullable = true)\n",
            " |-- Wind Speed (m/s): double (nullable = true)\n",
            " |-- Cloudiness (%): integer (nullable = true)\n",
            " |-- Rain (1h mm): double (nullable = true)\n",
            " |-- Sunrise: timestamp (nullable = true)\n",
            " |-- Sunset: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Select features\n",
        "# feature_columns = [\"Temperature (C)\", \"Humidity (%)\", \"Wind Speed (m/s)\", \"Cloudiness (%)\", \"Rain (1h mm)\"]\n",
        "# assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# # Transform dataset\n",
        "# final_data = assembler.transform(data).select(\"features\", \"Consumption (MW)\")"
      ],
      "metadata": {
        "id": "OF0cZGY9iqxt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "AWaMEk_Zh-FO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the model\n",
        "# gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Consumption (MW)\", maxIter=50)\n",
        "\n",
        "# # Train the model\n",
        "# model = gbt.fit(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "3BdqHnDoh_Y9",
        "outputId": "d39b2e61-ef9e-4b49-daf3-4dd43304d7a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ab1ec9541686>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Make predictions\n",
        "# predictions = model.transform(test_data)\n",
        "\n",
        "# # Evaluate predictions\n",
        "# evaluator = RegressionEvaluator(labelCol=\"Consumption (MW)\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "# rmse = evaluator.evaluate(predictions)\n",
        "# print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "metadata": {
        "id": "SeJcBKPuiAsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}